---
title: 机器学习基础总结二
date: 2018-12-20 17:57:22
tags: Machine Learning
mathjax: true
---

# 机器学习基础内容

## 机器学习三大构成部分
* Data

* Model

* Loss  

## Loss
衡量预测值与真实值之间的差异函数，机器学习的目的就是尽可能让Loss值降低，即提高自己预测的准确度，提高模型的鲁棒性。  
![两种常见的Loss函数](image.1)

## 有监督学习(Supervised learning)和无监督学习(Unsupervised learning)  
有监督和无监督学习是机器学习的两种办法(还有半监督学习，即强化学习)。  

**有监督学习**有标记，在分类任务中很重要。也是**决策树**和**神经网络**最常见的技术。常见的有监督学习算法：回归分析和统计分类。最典型的算法是**KNN**和**SVM**  

### 监督学习总的来说就是： regression & classification (回归和分类)  

**无监督学习**无标记，用于聚类， PCA和很多deep learning算法都属于无监督学习。

## Linear Regression(线性回归)
### *机器学习中向量默认是列向量*  

给定由 $d$ 个属性描述的示例 $x=(x_1;x_2;x_3...;x_d)$，其中 $x_i$ 是第 $i$ 个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即：

$$\Large f(\bm x)=w_1x_1+w_2x_2+...w_dx_d+b$$

用向量形式可写成：

$$\Large f(\bm x)=\bm{w}^T\bm{x}+b$$  

其中$\bm w=(w_1;w_2;...;w_d)$，$\bm w$ 和 b 学得之后，模型就可以确定。如何确定这两个参数呢？基本上分为两种办法：
* 闭式解
* 梯度下降

## w和b**最优解**的闭式解
证明过程直接贴图吧，建议自己证明一遍  
回归的性能度量方式——最小二乘法(又称最小平方法)：  
![最小二乘法](image3.PNG)  
![最小二乘法矩阵表示](image2.PNG)  

**w*的最优闭式解表示为：**  

$$\Large w*=(X^TX)^{-1}X^Ty=argmin_w\space L_D(w)$$

证明：  
![step1](image4.PNG)  
![step2](image5.PNG)
![step3](image6.PNG)  

然而闭式解有两个缺陷：  
* 很多矩阵不可逆  
* 很多大型矩阵的求逆计算量很大，时间复杂度为$O(n^3)$

## 梯度下降方法
作为一阶最优化方法，要找到一个函数**局部最小值**，就必须在函数当前点的**反方向**，用**规定步长**进行迭代搜索。*每次迭代的步长是可以改变的。*  
梯度下降法处理一些复杂的非线性函数会出现问题。而且靠近极小值时速度会减慢。  
通过尽可能降低Loss函数来获得一个最优的w。而最优化的方向我们定义为：  

$$\Large d=-\frac{\partial{L(w)}}{\partial w}$$

一次梯度下降的过程:  
![梯度下降过程](image7.PNG)  

$\eta$称为学习率，学习率要适合，不能过大也不能过小：  
* $\eta$过大，曲线容易振荡和diverge
* $\eta$过小，又收敛(converge)的太慢  

设置一个合适的学习率来让loss完美的收敛，一般可以这样做：

* 刚开始训练的时候设置比较大的学习率，这样曲线下降的快
* 在之后的epoch使用小学习率，接近结束时，衰减率应达到100倍以上
* 学习率的衰减可满足：$\eta^{t+1}=\displaystyle\frac{\eta^t}{t+1}$