---
title: 机器学习基础总结一
date: 2018-12-18 16:17:01
tags: Machine Learning
mathjax: true
---
 
 # **机器学习前置知识**

 ## **首先我们学习贝叶斯定理**
 需要掌握一些概率论的基本公式  

 加法公式：$\Large{P(A)=\underset{B}{\sum}P(A,B)}$

 乘法公式：$\Large{P(A,B)=P(B|A)P(A)}$  

 全概率公式：$\Large{P(A)=\underset{B}{\sum}{P(A|B)P(B)}}$  
  
 贝叶斯公式：$\Large{P(A|B)=\displaystyle\frac{P(B|A)P(A)}{P(B)}}$  

 贝叶斯定理是用来描述条件概率的，就比如${P(A|B)}$的意思就是，A，B两个独立事件，在B发生的情况下A发生的概率。  
 

 事件A发生的条件下事件B发生的概率等于事件A,B同时发生的概率与事件A发生概率的比值。

 ## **将贝叶斯公式运用到机器学习中**  

 $$\Large{P(“属于某类”|“具有某特征”)=\displaystyle\frac{P(“具有某特征”|“属于某类”)P(“属于某类”)}{P(“具有某特征”)}}$$

 当$P(“具有某特征”|“属于某类”)$很容易得出的时候，用这种方法就很方便。

 ## **将贝叶斯定理运用到贝叶斯学习中**
 
贝叶斯学习的目的是寻找**最优假设**，即通过给定的训练数据(已知的)来找到最有可能的假设。
即：  

$$\Large{argmax\space\space\ P(h|D)=\displaystyle\frac{P(D|h)P(h)}{P(D)}}$$  
也就是要求能使得$P(h|D)$最大时候的自变量值，$h$  
对于不同的自变量，$P(D)$是不变的，所以问题简化为求$argmax_h\space\ P(D|h)P(h)$，这个式子称为最大后验假设(Maximum a Posteriori hypothesi)：  

$$\Large{h_{MAP}=argmax_h\space P(D|h)P(h)}$$  
因为$P(h)$很难求，问题进一步转化为求$argmax_h\space\ P(D|h)$，这个式子称为最大似然假设(Maximum Likelihood (ML) hypothesis)：  

$$\Large{h_{ML}}=argmax_h\space P(D|h)$$

## **贝叶斯网络**
贝叶斯网络是个**有向无环**图，由原因指向结果，显然结果是由所指向的原因导致的。  
![一个贝叶斯网络](image1.PNG)

根据贝叶斯网络可以推算出联合概率  
![example](image2.PNG)  

## 概率与信息论  
信息论的基本想法是，一件不太可能发生的事件发生了，较之于一件很容易发生的事情，能提供更多的信息。所以某事件A的信息与发生概率呈负相关。    
* 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。

* 较不可能发生的事件具有更高的信息量。

* 独立事件应具有**增量**的信息。 例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。  

*事件A的信息度量公式：*  

$$\Large{I(A)=-\log_bP(A)}$$  
b做底，通常取b=2，也可以取3，10，e  

## 熵(entropy)
熵指接受的每条消息中包含信息的平均量

$$\Large{H(A)=E(I(A))=-E(\log_bP(A))=-\sum_AP(A)\log_bP(A)}$$  
E即期望，当$\displaystyle{P(A)=\frac{1}{n}}$的时候，$H(A)$为最大值。  
联合熵和条件熵也可推导：  
![联合熵与条件熵](image3.PNG)  

## 下一章进入机器学习正式内容~